{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b8c4a1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7dbbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "# additional torch packages\n",
    "import torch.nn.init as init\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from sklearn import metrics\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293077b3",
   "metadata": {},
   "source": [
    "## Import method, PIL, to get images as Tensors\n",
    "As there were issues converting to Tensors when using Rasterio for import, due to data types not matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f69dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 501, 501])\n"
     ]
    }
   ],
   "source": [
    "path = 'data'\n",
    "\n",
    "images_tensors = []\n",
    "for subdirectory in os.listdir(path):\n",
    "    subdirectory_path = os.path.join(path, subdirectory)\n",
    "    single_image = Image.open(subdirectory_path)\n",
    "    single_image = ToTensor()(single_image)\n",
    "    images_tensors.append(single_image)\n",
    "\n",
    "print(len(images_tensors))\n",
    "print(type(images_tensors))\n",
    "print(type(images_tensors[0]))\n",
    "print(images_tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b37897c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 501, 501])\n"
     ]
    }
   ],
   "source": [
    "path = 'labels'\n",
    "\n",
    "labels_tensors = []\n",
    "for subdirectory in os.listdir(path):\n",
    "    subdirectory_path = os.path.join(path, subdirectory)\n",
    "    single_label = Image.open(subdirectory_path)\n",
    "    single_label = ToTensor()(single_label)\n",
    "    labels_tensors.append(single_label)\n",
    "\n",
    "print(len(labels_tensors))\n",
    "print(type(labels_tensors))\n",
    "print(type(labels_tensors[0]))\n",
    "print(labels_tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_tensors[0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation and preparation\n",
    "\n",
    "* Reshape images to 256x256 pixel\n",
    "* Normalize images\n",
    "* Create data set\n",
    "* Split in training, validation and test data\n",
    "* Create data loader\n",
    "\n",
    "In the train data loader, we keep the shuffle parameter True since we want samples from all classes to be uniformly present in a batch which is important for optimal learning and convergence of batch gradient-based optimization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.CenterCrop((256, 256))\n",
    "images_cropped = [transform(image) for image in images_tensors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.CenterCrop((256, 256))\n",
    "images_cropped = [transform(image) for image in images_tensors]\n",
    "# convert images_256 to float tensor\n",
    "images_256 = [image.type(torch.FloatTensor) for image in images_cropped]\n",
    "labels_256 = [transform(label) for label in labels_tensors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_256[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "labels_one_hot = [F.one_hot(label.squeeze().long(), num_classes=3).permute(2, 0, 1) for label in labels_256]\n",
    "\n",
    "# Stack the one-hot encoded labels together\n",
    "labels_stacked = torch.stack(labels_one_hot)\n",
    "\n",
    "print(labels_stacked.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_stacked[2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pixel = torch.max(torch.stack(images_256[0:400]))\n",
    "# Normalize the images\n",
    "images_normalized = [image / max_pixel for image in images_256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images and labels:  400\n",
      "Number of validation images and labels:  50\n",
      "Number of test images and labels:  50\n"
     ]
    }
   ],
   "source": [
    "# Build data sets\n",
    "\n",
    "# Build Tensor dataset\n",
    "dataset = TensorDataset(torch.stack(images_normalized), labels_stacked)\n",
    "\n",
    "# Split in train (80%), validation (10%) and test (10%) sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Number of train images and labels: \", len(train_set))\n",
    "print(\"Number of validation images and labels: \", len(val_set))\n",
    "print(\"Number of test images and labels: \", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images in a batch in train_loader:  torch.Size([8, 1, 256, 256])\n",
      "Shape of labels in a batch in train_loader:  torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Build data loader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True) # shuffle training set\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False) # no need to shuffle validation set\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) # no need to shuffle test set\n",
    "\n",
    "# print shape of first batch of images in train_loader\n",
    "for images, labels in train_loader:\n",
    "    print(\"Shape of images in a batch in train_loader: \", images.shape)\n",
    "    print(\"Shape of labels in a batch in train_loader: \", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d0ebc",
   "metadata": {},
   "source": [
    "## Build U-Net Model\n",
    "\n",
    "Follow this example: https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3 \n",
    "\n",
    "$\\textbf{Number of Classes:}$\n",
    "\n",
    "3 classes:\n",
    "* Bright/white: The bright phase is nickel\n",
    "* Gray: The gray phase is YSZ\n",
    "* Dark: Pores and free space around the samples are dark\n",
    "\n",
    "$\\textbf{Encoder:}$\n",
    "\n",
    "The encoder is a series of convolutional and pooling layers that progressively downsample the input image to extract features at multiple scales.\n",
    "\n",
    "In the Encoder, the size of the image is gradually reduced while the depth gradually increases. This basically means the network learns the “WHAT” information in the image, however, it has lost the “WHERE” information.\n",
    "\n",
    "Input image: (1 x 501 x 501)\n",
    "\n",
    "* In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "* Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "\n",
    "$\\textbf{Decoder:}$\n",
    "\n",
    "The decoder consists of a series of convolutional and upsampling layers that upsample the feature maps to the original input image size while also incorporating the high-resolution features from the encoder. This allows the decoder to produce segmentation masks that have the same size as the original input image.\n",
    "\n",
    "In the Decoder, the size of the image gradually increases while the depth gradually decreases. This basically means the network learns the “WHERE” information in the image, by gradually applying up-sampling.\n",
    "\n",
    "$\\textbf{Final Layer:}$\n",
    "\n",
    "At the final layer, a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes.\n",
    "\n",
    "$\\textbf{Forward method:}$\n",
    "\n",
    "The forward method specifies how the input is processed through the network. The input image is first passed through the encoder layers to extract the features. Then, the decoder layers are used to upsample the features to the original image size while concatenating the corresponding encoder feature maps. Finally, the output layer uses a 1x1 convolutional layer to map the features to the desired number of output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3d3a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (e11): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (outconv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (downsampling)\n",
    "        # input size: (1, 501, 501)\n",
    "        self.e11 = Conv2d(1, 64, kernel_size=3, padding=1) # output: (64, 499, 499)\n",
    "        self.e12 = Conv2d(64, 64, kernel_size=3, padding=1) # output: (64, 497, 497)\n",
    "        self.pool1 = MaxPool2d(kernel_size=2, stride=2) # output: (64, 249, 249)\n",
    "\n",
    "        # input size: (64, 249, 249)\n",
    "        self.e21 = Conv2d(64, 128, kernel_size=3, padding=1) # output: (128, 247, 247)\n",
    "        self.e22 = Conv2d(128, 128, kernel_size=3, padding=1) # output: (128, 245, 245)\n",
    "        self.pool2 = MaxPool2d(kernel_size=2, stride=2) # output: (128, 122, 122)\n",
    "\n",
    "        # input size: (128, 122, 122)\n",
    "        self.e31 = Conv2d(128, 256, kernel_size=3, padding=1) # output: (256, 120, 120)\n",
    "        self.e32 = Conv2d(256, 256, kernel_size=3, padding=1) # output: (256, 118, 118)\n",
    "        self.pool3 = MaxPool2d(kernel_size=2, stride=2) # output: (256, 59, 59)\n",
    "\n",
    "        # input size: (256, 59, 59)\n",
    "        self.e41 = Conv2d(256, 512, kernel_size=3, padding=1) # output: (512, 57, 57)\n",
    "        self.e42 = Conv2d(512, 512, kernel_size=3, padding=1) # output: (512, 55, 55)\n",
    "        self.pool4 = MaxPool2d(kernel_size=2, stride=2) # output: (512, 27, 27)\n",
    "\n",
    "        # input size: (512, 27, 27)\n",
    "        self.e51 = Conv2d(512, 1024, kernel_size=3, padding=1) # output: (1024, 25, 25)\n",
    "        self.e52 = Conv2d(1024, 1024, kernel_size=3, padding=1) # output: (1024, 23, 23)\n",
    "\n",
    "        # Decoder (upsampling)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        #xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xu22 = torch.cat([xu2, xe32[:, :, :xu2.size(2), :xu2.size(3)]], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22[:, :, :xu3.size(2), :xu3.size(3)]], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12[:, :, :xu4.size(2), :xu4.size(3)]], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = Unet(3) # 3 classes\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba014fb5",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86e5ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function: Cross entropy loss (for multi-class classification)\n",
    "# What output activation function should we use?\n",
    "loss_unet = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer: ADAM\n",
    "optimizer_unet = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e24f6",
   "metadata": {},
   "source": [
    "## Test forward pass on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fedc4277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(8, 1, 256, 256)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    if use_cuda:\n",
    "        dummy_input = dummy_input.cuda()\n",
    "    output = net(dummy_input)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/unet_soren_test.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/unet_soren_test.ipynb#X45sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m output \u001b[39m=\u001b[39m net(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/unet_soren_test.ipynb#X45sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/unet_soren_test.ipynb#X45sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss_unet(output, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/unet_soren_test.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# clean up gradients from previous run\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/unet_soren_test.ipynb#X45sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer_unet\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1\n",
    "\n",
    "# Steps\n",
    "train_steps = len(train_set)//batch_size\n",
    "val_steps = len(val_set)//batch_size\n",
    "test_steps = len(test_set)//batch_size\n",
    "\n",
    "# validation loss\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "# lists to store training and validation losses\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# lists to store training and validation accuracies\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "net.train()\n",
    "\n",
    "# start time (for printing elapsed time per epoch)\n",
    "starttime = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # loop over training data\n",
    "    for images, labels in train_loader:\n",
    "        # send the input to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.to(torch.float32)\n",
    "        \n",
    "        # Complete forward pass through model\n",
    "        output = net(images)\n",
    "        \n",
    "        # Compute the loss\n",
    "        train_loss = loss_unet(output, labels)\n",
    "\n",
    "        # clean up gradients from previous run\n",
    "        optimizer_unet.zero_grad()\n",
    "\n",
    "        # Compute gradients using back propagation\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Take a step with the optimizer to update the weights\n",
    "        optimizer_unet.step()\n",
    "\n",
    "        # add the loss to the training set's running loss\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        # set the model to evaluation mode\n",
    "        net.eval()\n",
    "\n",
    "        # loop over validation data\n",
    "        for images, labels in val_loader:\n",
    "            # send the input to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.to(torch.float32)\n",
    "\n",
    "            # Complete forward pass through model\n",
    "            output = net(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            val_loss = loss_unet(output, labels)\n",
    "\n",
    "            # add the loss to the validation set's running loss \n",
    "            total_val_loss += val_loss\n",
    "\n",
    "    # print training/validation statistics\n",
    "    avg_train_loss = total_train_loss/train_steps\n",
    "    avg_val_loss = total_val_loss/val_steps\n",
    "\n",
    "    # update training history\n",
    "    #train_losses.append(avg_train_loss.cpu().numpy())\n",
    "    train_losses.append(avg_train_loss.cpu().detach().numpy())\n",
    "    validation_losses.append(avg_val_loss.cpu().numpy())\n",
    "\n",
    "    # print training/validation statistics\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, avg_train_loss, avg_val_loss))\n",
    "    \n",
    "# display time elapsed for epoch\n",
    "endtime = time.time()\n",
    "print(f\"Elapsed time: {(endtime - starttime)/60:.2f} min\")\n",
    "print(\"Finished Training\")\n",
    "\n",
    "plt.plot(np.linspace(1, n_epochs, 25), train_losses, 'b', label='Training loss')\n",
    "plt.plot(np.linspace(1, n_epochs, 25), validation_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('figures/loss.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
