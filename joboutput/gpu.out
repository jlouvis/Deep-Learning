loaded images and labels
data sets built
Unet(
  (e11): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn51): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn52): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))
  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn11d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn12d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn21d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn22d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn31d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn32d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn41d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn42d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (outconv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))
)
Epoch: 1 	Training Loss: 0.617844 	Validation Loss: 0.674963
Epoch: 2 	Training Loss: 0.588674 	Validation Loss: 0.680452
Epoch: 3 	Training Loss: 0.584773 	Validation Loss: 0.676812
Epoch: 4 	Training Loss: 0.583779 	Validation Loss: 0.677391
Epoch: 5 	Training Loss: 0.583623 	Validation Loss: 0.677377
Epoch: 6 	Training Loss: 0.583555 	Validation Loss: 0.677064
Epoch: 7 	Training Loss: 0.583351 	Validation Loss: 0.676249
Epoch: 8 	Training Loss: 0.583352 	Validation Loss: 0.676514
Epoch: 9 	Training Loss: 0.583657 	Validation Loss: 0.675990
Epoch: 10 	Training Loss: 0.583503 	Validation Loss: 0.675662
Epoch: 11 	Training Loss: 0.583319 	Validation Loss: 0.677830
Epoch: 12 	Training Loss: 0.583319 	Validation Loss: 0.675220
Epoch: 13 	Training Loss: 0.583239 	Validation Loss: 0.676593
Epoch: 14 	Training Loss: 0.583108 	Validation Loss: 0.673994
Epoch: 15 	Training Loss: 0.583057 	Validation Loss: 0.673913
Epoch: 16 	Training Loss: 0.583203 	Validation Loss: 0.674937
Epoch: 17 	Training Loss: 0.583418 	Validation Loss: 0.676907
Epoch: 18 	Training Loss: 0.583140 	Validation Loss: 0.674163
Epoch: 19 	Training Loss: 0.583040 	Validation Loss: 0.674184
Epoch: 20 	Training Loss: 0.583175 	Validation Loss: 0.674455
Epoch: 21 	Training Loss: 0.583097 	Validation Loss: 0.673590
Epoch: 22 	Training Loss: 0.583194 	Validation Loss: 0.672264
Epoch: 23 	Training Loss: 0.583320 	Validation Loss: 0.673893
Epoch: 24 	Training Loss: 0.583189 	Validation Loss: 0.671336
Epoch: 25 	Training Loss: 0.583314 	Validation Loss: 0.671050
Elapsed time: 9.36 min
Finished Training
