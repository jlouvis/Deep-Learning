{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b8c4a1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7dbbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "# additional torch packages\n",
    "import torch.nn.init as init\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from sklearn import metrics\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293077b3",
   "metadata": {},
   "source": [
    "## Import method, PIL, to get images as Tensors\n",
    "As there were issues converting to Tensors when using Rasterio for import, due to data types not matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'data'\n",
    "label_path = 'labels'\n",
    "\n",
    "images_tensors = []\n",
    "labels_tensors = []\n",
    "\n",
    "# Getting a sorted list of filenames from both image and label directories\n",
    "image_files = sorted(os.listdir(image_path))\n",
    "label_files = sorted(os.listdir(label_path))\n",
    "\n",
    "# Zipping the sorted filenames so they correspond to each other\n",
    "for img_filename, lbl_filename in zip(image_files, label_files):\n",
    "    if img_filename[9:12] == lbl_filename[7:10]:  # Matching filenames by position\n",
    "        img_filepath = os.path.join(image_path, img_filename)\n",
    "        lbl_filepath = os.path.join(label_path, lbl_filename)\n",
    "\n",
    "        # Reading and converting images to tensors\n",
    "        single_image = Image.open(img_filepath)\n",
    "        single_image = ToTensor()(single_image)\n",
    "        images_tensors.append(single_image)\n",
    "\n",
    "        # Reading and converting labels to tensors\n",
    "        single_label = Image.open(lbl_filepath)\n",
    "        single_label = ToTensor()(single_label)\n",
    "        labels_tensors.append(single_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29efdf",
   "metadata": {},
   "source": [
    "## Visualize first 10 images and their labels\n",
    "\n",
    "Comment: Somehow it seems like the gray scale changes the order of the colors. Where they were white before, they are now black?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 5 images and first 5 labels in a 2x5 subplot\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 4))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(images_tensors[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title('Image')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(labels_tensors[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title('Label')\n",
    "    axes[1, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation and preparation\n",
    "\n",
    "* Reshape images to 256x256 pixel\n",
    "* Normalize images\n",
    "* Create data set\n",
    "* Split in training, validation and test data\n",
    "* Create data loader\n",
    "\n",
    "In the train data loader, we keep the shuffle parameter True since we want samples from all classes to be uniformly present in a batch which is important for optimal learning and convergence of batch gradient-based optimization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images and labels to 256x256 and convert images to float\n",
    "transform = transforms.CenterCrop((256, 256))\n",
    "images_cropped = [transform(image) for image in images_tensors]\n",
    "images_256 = [image.type(torch.FloatTensor) for image in images_cropped]\n",
    "labels_cropped = [transform(label) for label in labels_tensors]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "labels_one_hot = [F.one_hot(label.squeeze().long(), num_classes=3).permute(2, 0, 1).float() for label in labels_cropped]\n",
    "\n",
    "# Stack the one-hot encoded labels together\n",
    "labels_stacked = torch.stack(labels_one_hot)\n",
    "\n",
    "max_pixel = torch.max(torch.stack(images_256[0:400]))\n",
    "# Normalize the images\n",
    "images_normalized = [image / max_pixel for image in images_256]\n",
    "\n",
    "# Build data sets\n",
    "\n",
    "# Build Tensor dataset\n",
    "dataset = TensorDataset(torch.stack(images_normalized), labels_stacked)\n",
    "\n",
    "# Split in train (80%), validation (10%) and test (10%) sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Build data loader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True) # shuffle training set\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False) # no need to shuffle validation set\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) # no need to shuffle test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d0ebc",
   "metadata": {},
   "source": [
    "## Build U-Net Model\n",
    "\n",
    "Follow this example: https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3 \n",
    "\n",
    "$\\textbf{Number of Classes:}$\n",
    "\n",
    "3 classes:\n",
    "* Bright/white: The bright phase is nickel\n",
    "* Gray: The gray phase is YSZ\n",
    "* Dark: Pores and free space around the samples are dark\n",
    "\n",
    "$\\textbf{Encoder:}$\n",
    "\n",
    "The encoder is a series of convolutional and pooling layers that progressively downsample the input image to extract features at multiple scales.\n",
    "\n",
    "In the Encoder, the size of the image is gradually reduced while the depth gradually increases. This basically means the network learns the “WHAT” information in the image, however, it has lost the “WHERE” information.\n",
    "\n",
    "Input image: (1 x 501 x 501)\n",
    "\n",
    "* In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "* Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "\n",
    "$\\textbf{Decoder:}$\n",
    "\n",
    "The decoder consists of a series of convolutional and upsampling layers that upsample the feature maps to the original input image size while also incorporating the high-resolution features from the encoder. This allows the decoder to produce segmentation masks that have the same size as the original input image.\n",
    "\n",
    "In the Decoder, the size of the image gradually increases while the depth gradually decreases. This basically means the network learns the “WHERE” information in the image, by gradually applying up-sampling.\n",
    "\n",
    "$\\textbf{Final Layer:}$\n",
    "\n",
    "At the final layer, a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes.\n",
    "\n",
    "$\\textbf{Forward method:}$\n",
    "\n",
    "The forward method specifies how the input is processed through the network. The input image is first passed through the encoder layers to extract the features. Then, the decoder layers are used to upsample the features to the original image size while concatenating the corresponding encoder feature maps. Finally, the output layer uses a 1x1 convolutional layer to map the features to the desired number of output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d3a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (e11): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn42): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn51): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn52): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn11d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn12d): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn21d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn22d): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn31d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn32d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn41d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn42d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (outconv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# with batch normalization\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        # Encoder (downsampling)\n",
    "        self.e11 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(64)\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(128)\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(256)\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(512)\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn51 = nn.BatchNorm2d(1024)\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        self.bn52 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        # Decoder (upsampling)\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.bn11d = nn.BatchNorm2d(512)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(256)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(128)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(64)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = F.relu(self.bn11(self.e11(x)))\n",
    "        xe12 = F.relu(self.bn12(self.e12(xe11)))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = F.relu(self.bn21(self.e21(xp1)))\n",
    "        xe22 = F.relu(self.bn22(self.e22(xe21)))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = F.relu(self.bn31(self.e31(xp2)))\n",
    "        xe32 = F.relu(self.bn32(self.e32(xe31)))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = F.relu(self.bn41(self.e41(xp3)))\n",
    "        xe42 = F.relu(self.bn42(self.e42(xe41)))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = F.relu(self.bn51(self.e51(xp4)))\n",
    "        xe52 = F.relu(self.bn52(self.e52(xe51)))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = F.relu(self.bn11d(self.d11(xu11)))\n",
    "        xd12 = F.relu(self.bn12d(self.d12(xd11)))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32[:, :, :xu2.size(2), :xu2.size(3)]], dim=1)\n",
    "        xd21 = nn.functional.relu(self.bn21d(self.d21(xu22)))\n",
    "        xd22 = nn.functional.relu(self.bn22d(self.d22(xd21)))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22[:, :, :xu3.size(2), :xu3.size(3)]], dim=1)\n",
    "        xd31 = F.relu(self.bn31d(self.d31(xu33)))\n",
    "        xd32 = F.relu(self.bn32d(self.d32(xd31)))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12[:, :, :xu4.size(2), :xu4.size(3)]], dim=1)\n",
    "        xd41 = F.relu(self.bn41d(self.d41(xu44)))\n",
    "        xd42 = F.relu(self.bn42d(self.d42(xd41)))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "        # restore output to original shape\n",
    "        out = F.interpolate(out, size=(501,501), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = Unet(3) # 3 classes\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba014fb5",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e5ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function: Cross entropy loss (for multi-class classification)\n",
    "# What output activation function should we use?\n",
    "loss_unet = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer: ADAM\n",
    "optimizer_unet = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e24f6",
   "metadata": {},
   "source": [
    "## Test forward pass on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fedc4277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 3, 496, 496])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(8, 1, 501, 501)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    if use_cuda:\n",
    "        dummy_input = dummy_input.cuda()\n",
    "    output = net(dummy_input)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output.shape)\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e22149",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55320fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/Assignment_X_Rays.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/Assignment_X_Rays.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer_unet\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/Assignment_X_Rays.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Compute gradients using back propagation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/Assignment_X_Rays.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/Assignment_X_Rays.ipynb#X31sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Take a step with the optimizer to update the weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sorenbendtsen/Documents/GitHub/Deep-Learning/Assignment_X_Rays.ipynb#X31sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m optimizer_unet\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# function to calculate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total = labels.size(0) * labels.size(1) * labels.size(2)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Initialize lists to store training and validation metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    net.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = get_variable(images), get_variable(labels)\n",
    "        optimizer_unet.zero_grad()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = loss_unet(outputs, labels.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer_unet.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_acc += accuracy(outputs, labels.argmax(dim=1)) * images.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_acc / len(train_loader.dataset)\n",
    "\n",
    "    # Store training metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = get_variable(images), get_variable(labels)\n",
    "            outputs = net(images)\n",
    "            loss = loss_unet(outputs, labels.argmax(dim=1))\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_acc += accuracy(outputs, labels.argmax(dim=1)) * images.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_acc / len(val_loader.dataset)\n",
    "\n",
    "    # Store validation metrics\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}]')\n",
    "    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Plot the train and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, n_epochs+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "# show val loss as text on plot for last epoch\n",
    "plt.text(n_epochs, val_losses[-1], f'{val_losses[-1]:.4f}')\n",
    "plt.savefig('figures/unet_loss.png')\n",
    "\n",
    "# Plot the train and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_epochs+1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, n_epochs+1), val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n",
    "# show val accuracy as text on plot for last epoch\n",
    "plt.text(n_epochs, val_accuracies[-1], f'{val_accuracies[-1]:.4f}')\n",
    "plt.savefig('figures/unet_accuracy.png')\n",
    "\n",
    "# For just 1 image, show the original, the label and the prediction side by side\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = get_variable(images), get_variable(labels)\n",
    "        outputs = net(images)\n",
    "        break\n",
    "    \n",
    "image = images[0].squeeze().cpu().numpy()\n",
    "label = labels[0].squeeze().cpu().numpy()\n",
    "output = outputs[0].squeeze().cpu().numpy()\n",
    "\n",
    "# Rearrange dimensions from (3, 256, 256) to (256, 256, 3)\n",
    "image = np.transpose(image, (1, 2, 0))  # If image shape is (3, 256, 256)\n",
    "label = np.transpose(label, (1, 2, 0))  # If label shape is (3, 256, 256)\n",
    "output = np.transpose(output, (1, 2, 0))  # If output shape is (3, 256, 256)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(label, cmap='gray')\n",
    "axes[1].set_title('Actual Label')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output, cmap='gray')\n",
    "axes[2].set_title('Predicted Label')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure to the 'Figures' folder\n",
    "plt.savefig('figures/unet_sample_prediction.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "net.eval() # prep model for evaluation\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # send the input to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.to(torch.float32)\n",
    "\n",
    "        # Complete forward pass through model\n",
    "        output = net(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_unet(output, labels)\n",
    "\n",
    "        # add the loss to the validation set's running loss \n",
    "        test_loss += loss\n",
    "\n",
    "        # get the predicted class from the maximum value in the output-list of class scores\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        # compare predictions to true label\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # store true and predicted labels\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_pred.append(predicted.cpu().numpy())\n",
    "\n",
    "# print test loss and accuracy\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "print('Test Accuracy: %d%% (%d/%d)\\n' % (100 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".1f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare original image with labels and predicted labels\n",
    "# get the first batch of images and labels\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# send the input to device\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "images = images.to(torch.float32)\n",
    "\n",
    "# Complete forward pass through model\n",
    "output = net(images)\n",
    "\n",
    "# get the predicted class from the maximum value in the output-list of class scores\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(8):\n",
    "    ax = fig.add_subplot(2, 8/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(images[idx][0].cpu().numpy(), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(predicted[idx].item(), labels[idx].item()),\n",
    "                 color=(\"green\" if predicted[idx]==labels[idx] else \"red\"))\n",
    "\n",
    "# plot the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(8):\n",
    "    ax = fig.add_subplot(2, 8/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(labels[idx][0].cpu().numpy(), cmap='gray')\n",
    "    ax.set_title(\"Label {}\".format(idx+1))\n",
    "\n",
    "# plot the corresponding predicted labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(8):\n",
    "    ax = fig.add_subplot(2, 8/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(predicted[idx][0].cpu().numpy(), cmap='gray')\n",
    "    ax.set_title(\"Predicted Label {}\".format(idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(images[j, 0, :, :], cmap='gray')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Ground Truth Label')\n",
    "        plt.imshow(labels_np[j, :, :], cmap='gray', vmin=0, vmax=3)  # adjust vmin and vmax based on your classes\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Predicted Output')\n",
    "        plt.imshow(output_np[j].argmax(axis=0), cmap='gray', vmin=0, vmax=3)  # adjust vmin and vmax based on your classes\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
